{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Good Example\n",
    "\\begin{align*}\n",
    "NM:\n",
    "\\begin{cases}\n",
    "& x_{n+1}=(I+\\eta \\left[\\begin{matrix}0.1 & 0.5 \\\\ 0 & 0.1 \\end{matrix}\\right]) x_n+\\sqrt{\\eta}\\left[\\begin{matrix}0.7 & -0.6 \\\\ 0 & 0.7 \\end{matrix}\\right]u_n, \\quad x_0=\\left[\\begin{matrix} 1 \\\\ -1 \\end{matrix}\\right]\\\\\n",
    "& y_n=\\left[\\begin{matrix}1 &0\\\\0 & 1 \\end{matrix}\\right]x_n+\\sigma_0 v_n, \\quad \\sigma_0=0.5\n",
    "\\end{cases}\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------\n",
    "# library loading\n",
    "#--------------------------------------\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "#%matplotlib inline\n",
    "#%config InlineBackend.figure_format = 'svg'\n",
    "#set suppress to not use scientific counting\n",
    "np.set_printoptions(suppress=True) \n",
    "\n",
    "#---------------------------------------\n",
    "# Initialization Math Model\n",
    "#---------------------------------------\n",
    "cpu_start=time.perf_counter()\n",
    "\n",
    "dimX=2; dimY=2; eta=0.005\n",
    "N=1000; n0=50; N_sample= 1000\n",
    "sigma0_train=0.5\n",
    "\n",
    "#Deterministic Matrix\n",
    "F0=np.array([[0.1,0.5],[0,0.1]])\n",
    "F=np.eye(dimX)+eta*F0\n",
    "G=np.sqrt(eta)*np.array([[0.7,-0.6],[0,0.7]])\n",
    "H=np.array([[1,0],[0,1]])\n",
    "x0=np.array([[1],[-1]])\n",
    "#Covariance Matrix for random variable\n",
    "Q0=np.eye(dimX) #covariance of random variable u_n\n",
    "R0=sigma0_train*sigma0_train*np.eye(dimY) #covariance of random variable v_n\n",
    "\n",
    "# generate u_n, v_n\n",
    "# 1-d Gaussian: np.random.default_rng().normal(mean, std, size)\n",
    "# n-d Gaussian: np.random.default_rng().multivariate_normal(mean,cov,size)\n",
    "# note to reshape multivariate normal random variable to column vector.\n",
    "\n",
    "rng=np.random.default_rng()\n",
    "u=[rng.multivariate_normal(np.zeros(dimX),np.eye(dimX),1).reshape(dimX,1) for i in range(N)]\n",
    "v=[rng.multivariate_normal(np.zeros(dimY),np.eye(dimY),1).reshape(dimY,1) for i in range(N+1)]\n",
    "u=np.array(u)\n",
    "v=np.array(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------\n",
    "# Monte Carlo Simulation for once\n",
    "#--------------------------------------\n",
    "def mc_simulation(F,G,H,u,v,sigma0_train,N):\n",
    "    \"\"\"Monte Carlo Simulation\n",
    "       N: time step horizon.\n",
    "       sigma0_train: observation noise.\"\"\"\n",
    "    x_raw=np.zeros((N+1,dimX,1)); x_raw[0]=x0                                     \n",
    "    y_raw=np.zeros((N+1,dimY,1)); y_raw[0]=H@x0+sigma0_train*v[0] #!!!\n",
    "    for k in range(N):\n",
    "        x_raw[k+1]=F@x_raw[k]+G@u[k]  #!!!\n",
    "        y_raw[k+1]=H@x_raw[k+1]+sigma0_train*v[k+1] #!!!\n",
    "    return x_raw, y_raw\n",
    "\n",
    "#---------------------------------------\n",
    "# Kalman Filtering Algorithm\n",
    "#---------------------------------------\n",
    "def kalman_filtering(F,G,H,Q0,R0,x0,y_raw,N):\n",
    "    \"\"\"Kalman Filtering Algorithm\"\"\"\n",
    "    #caution: need to specific x is dimX x 1 to be column vector\n",
    "    x_hat=np.zeros((N+1,dimX,1)); x_hat[0]=x0\n",
    "    R=np.zeros((N+1,dimX,dimX)); R[0]=np.zeros((dimX,dimX))  #!!!\n",
    "    \n",
    "    for k in range(N):\n",
    "        #y_raw has to be column array or vector.\n",
    "        inv=np.linalg.inv(H@R[k]@H.T+R0)\n",
    "        x_hat[k+1]=F@x_hat[k]+F@R[k]@H.T@inv@(y_raw[k]-H@x_hat[k]) #!!!\n",
    "        R[k+1]=F@(R[k]-R[k]@H.T@inv@H@R[k])@F.T+G@Q0@G.T           #!!!\n",
    "        \n",
    "    x_bar=[x_hat[k]+R[k]@H.T@np.linalg.inv(H@R[k]@H.T+R0)@(y_raw[k]-H@x_hat[k]) for k in range(N+1)]\n",
    "    x_bar=np.array(x_bar) #make list to np.array\n",
    "    \n",
    "    return x_hat, x_bar\n",
    "\n",
    "#-----------------------------\n",
    "# Generating tons of samples\n",
    "#-----------------------------\n",
    "\n",
    "def sample_generator(Q0,R0,sigma0_train):\n",
    "    \"\"\"sigma0_train: observation noise.\"\"\"\n",
    "    datas=np.zeros(((N-n0+2)*N_sample,n0,dimY)) #for each sample path, we have N-n0+2 data\n",
    "    labels=np.zeros(((N-n0+2)*N_sample,dimX))\n",
    "    \n",
    "    x_bars=np.zeros(((N-n0+2)*N_sample,dimX)) #store Kalman filtering estimation value.\n",
    "    x_hats=np.zeros(((N-n0+2)*N_sample,dimX))\n",
    "    \n",
    "    x_raws=np.zeros((N_sample, N+1, dimX, 1))\n",
    "    y_raws=np.zeros((N_sample, N+1, dimY, 1))\n",
    "    \n",
    "    for i in range(N_sample):\n",
    "        data=np.zeros((N-n0+2,n0,dimY)) #store data for each sample\n",
    "        label=np.zeros((N-n0+2,dimX))\n",
    "        # call mc_simulation function to generate sample\n",
    "        x_raw,y_raw=mc_simulation(F,G,H,u,v,sigma0_train,N)\n",
    "        x_raws[i]=x_raw; y_raws[i]=y_raw\n",
    "        \n",
    "        # call kalman_filtering function to compute estimation\n",
    "        # make sure here y_raw to be column vector\n",
    "        x_hat, x_bar=kalman_filtering(F,G,H,Q0,R0,x0,y_raw,N)\n",
    "        \n",
    "        # convert x_raw...into row vector\n",
    "        x_raw=x_raw.reshape(N+1,dimX) \n",
    "        y_raw=y_raw.reshape(N+1,dimY)\n",
    "        x_hat=x_hat.reshape(N+1,dimX)\n",
    "        x_bar=x_bar.reshape(N+1,dimX)\n",
    "        \n",
    "        # make data and label for each sample\n",
    "        for k in range(N-n0+2):\n",
    "            data[k]=y_raw[k:k+n0]\n",
    "            label[k]=x_raw[k+n0-1]\n",
    "            \n",
    "        # put data and label into datas and labels with i representing sample number\n",
    "        datas[i*(N-n0+2):(i+1)*(N-n0+2)]=data\n",
    "        labels[i*(N-n0+2):(i+1)*(N-n0+2)]=label\n",
    "        x_hats[i*(N-n0+2):(i+1)*(N-n0+2)]=x_hat[n0-1:]\n",
    "        x_bars[i*(N-n0+2):(i+1)*(N-n0+2)]=x_bar[n0-1:]\n",
    "    \n",
    "    return datas,labels,x_hats,x_bars,x_raws,y_raws"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# call sample_generator function to generate sample\n",
    "#datas, labels, x_hats,x_bars,x_raws, y_raws=sample_generator(Q0,R0,sigma0_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#-------------------------------\n",
    "# Deep Filtering Function\n",
    "#--------------------------------\n",
    "def deep_filtering(datas,labels,x_hats,x_bars,x_raws,y_raws):\n",
    "    \"\"\"datas,labels,x_hats,x_bars,x_raw,y_raws\"\"\"\n",
    "    # Data Preprocessing Procedure\n",
    "    datas=datas.reshape(((N-n0+2)*N_sample,dimY*n0))\n",
    "    # convert numpy array into pandas dataframe\n",
    "    datas=pd.DataFrame(datas)\n",
    "    labels=pd.DataFrame(labels)\n",
    "    x_hats=pd.DataFrame(x_hats)\n",
    "    x_bars=pd.DataFrame(x_bars)\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    seed1=3\n",
    "    np.random.seed(seed1)\n",
    "    training_data, test_data, training_label, test_label=train_test_split(datas,labels, test_size=0.2, random_state=seed1)\n",
    "    \n",
    "    # input data normalization\n",
    "    data_mean=training_data.mean(axis=0)\n",
    "    data_std=training_data.std(axis=0)\n",
    "\n",
    "    training_data=(training_data-data_mean)/data_std\n",
    "    test_data=(test_data-data_mean)/data_std\n",
    "\n",
    "    # output data normalization\n",
    "    #label_mean=training_label.mean(axis=0)\n",
    "    #label_std=training_label.std(axis=0)\n",
    "\n",
    "    #training_label=(training_label-label_mean)/label_std\n",
    "    #test_label=(test_label-label_mean)/label_std\n",
    "\n",
    "    #-------------------------------\n",
    "    # DNN Model building\n",
    "    #-------------------------------\n",
    "\n",
    "    from keras import models\n",
    "    from keras import layers\n",
    "    from keras import optimizers\n",
    "    \n",
    "    def build_model():\n",
    "        model=models.Sequential()\n",
    "        model.add(layers.Dense(5,activation='relu',input_shape=(dimY*n0,)))\n",
    "        model.add(layers.Dense(5,activation='relu'))\n",
    "        model.add(layers.Dense(5,activation='relu'))\n",
    "        model.add(layers.Dense(5,activation='relu'))\n",
    "        model.add(layers.Dense(5,activation='relu'))\n",
    "        model.add(layers.Dense(dimX))\n",
    "        model.compile(optimizer=optimizers.SGD(lr=0.001), \n",
    "                  loss='mean_squared_error', \n",
    "                  metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "        return model\n",
    "    \n",
    "    model=build_model()\n",
    "    mymodel=model.fit(training_data,training_label,epochs=10, batch_size=8)\n",
    "    \n",
    "    #-------------------------------\n",
    "    # Evaluation Performance\n",
    "    #-------------------------------\n",
    "    from sklearn.metrics import mean_squared_error\n",
    "    test_mse_score, test_mae_score =model.evaluate(test_data,test_label)\n",
    "    \n",
    "    # Normalization x_bars data to compare with test_label\n",
    "    #x_bars=(x_bars-label_mean)/label_std\n",
    "    # find test_label index in DataFrame\n",
    "    index=test_label.index.tolist()\n",
    "    kf_mse_err=mean_squared_error(x_bars.iloc[index],test_label)\n",
    "    cpu_end=time.perf_counter()\n",
    "\n",
    "    #print(\"The mse of deep filtering is {:.3%}\".format(test_mse_score))\n",
    "    #print(\"The mse of Kalman Filtering is {:.3%}\".format(kf_mse_err))\n",
    "    #print(\"The CPU consuming time is {:.5}\".format(cpu_end-cpu_start))\n",
    "    #-------------------------------\n",
    "    # Training loss graph\n",
    "    #-------------------------------\n",
    "    #history_dict=mymodel.history\n",
    "    #loss_value=history_dict['loss']\n",
    "    #val_loss_value=history_dict['val_loss']\n",
    "    #epochs=range(1,10+1)\n",
    "    #import matplotlib.pyplot as plt\n",
    "    #plt.plot(epochs, loss_value, 'bo',label='Training Loss')\n",
    "    #plt.plot(epochs, val_loss_value,'b',label='Validation Loss')\n",
    "    #plt.legend()\n",
    "    #plt.show()\n",
    "    \n",
    "    return model,data_mean,data_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------\n",
    "# plot on new data\n",
    "#----------------------------------------\n",
    "def graph_plot(N_new):\n",
    "    \"\"\"N_new: new number of time step horizon\"\"\"\n",
    "    x_new,y_new=mc_simulation(F,G,H,u,v,sigma0_train,N_new)\n",
    "    x_hat_new,x_bar_new=kalman_filtering(F,G,H,Q0,R0,x0,y_new,N_new)\n",
    "    y_new=y_new.reshape(N_new+1,dimY)\n",
    "    data_new=np.zeros((N_new-n0+2,n0,dimY))\n",
    "    for k in range(N_new-n0+2):\n",
    "        data_new[k]=y_new[k:k+n0]\n",
    "    data_new=data_new.reshape(N_new-n0+2,n0*dimY)\n",
    "    data_new=pd.DataFrame(data_new)\n",
    "    # input normalization\n",
    "    data_new=(data_new-data_mean)/data_std\n",
    "    # deep filtering prediction value.\n",
    "    df_pred=model.predict(data_new)\n",
    "    # convert prediction value to original scale.\n",
    "    #for i in range(N_new-n0+2):\n",
    "    #    df_pred[i,:]=df_pred[i,:]*label_mean+label_std\n",
    "    #--------------------------------\n",
    "    # For estimation before state index n0-1, we use x0 to replace it.\n",
    "    df_new=[x0 for k in range(n0-1)]\n",
    "    df_new=np.array(df_new)\n",
    "    df_new=df_new.reshape(n0-1,dimX)\n",
    "    df_new=np.vstack((df_new,df_pred))\n",
    "    \n",
    "    #--------------------------------\n",
    "    axis=np.linspace(0,5,N_new+1)\n",
    "    fig,ax=plt.subplots(2,1,figsize=[8,12])\n",
    "    ax[0].plot(axis, x_new[:,0],'c',axis,x_bar_new[:,0],'b',axis,df_new[:,0],'r',linewidth=0.5)\n",
    "    ax[0].minorticks_on()\n",
    "    ax[0].set_xlim((0,5))\n",
    "    ax[1].plot(axis, x_new[:,1],'c',axis, x_bar_new[:,1],'b',axis,df_new[:,1],'r',linewidth=0.6)\n",
    "    ax[1].set_xlim((0,5))\n",
    "    ax[1].minorticks_on()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Dependece Analysis\n",
    "\n",
    "\\begin{align*}\n",
    "NM:\n",
    "\\begin{cases}\n",
    "& x_{n+1}=(I+\\eta \\left[\\begin{matrix}0.1 & 0.5 \\\\ 0 & 0.1 \\end{matrix}\\right]) x_n+\\sqrt{\\eta}\\left[\\begin{matrix}0.7 & -0.6 \\\\ 0 & 0.7 \\end{matrix}\\right]u_n, \\quad x_0=\\left[\\begin{matrix} 1 \\\\ -1 \\end{matrix}\\right]\\\\\n",
    "& y_n=\\left[\\begin{matrix}1 &0\\\\0 & 1 \\end{matrix}\\right]x_n+\\sigma_0^{NM} v_n\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "AM:\n",
    "\\begin{cases}\n",
    "& x_{n+1}=(I+\\eta \\left[\\begin{matrix}0.1 & 0.5 \\\\ 0 & 0.1 \\end{matrix}\\right]) x_n+\\sqrt{\\eta}\\left[\\begin{matrix}0.7 & -0.6 \\\\ 0 & 0.7 \\end{matrix}\\right]u_n, \\quad x_0=\\left[\\begin{matrix} 1 \\\\ -1 \\end{matrix}\\right]\\\\\n",
    "& y_n=\\left[\\begin{matrix}1 &0\\\\0 & 1 \\end{matrix}\\right]x_n+\\sigma_0^{AM} v_n\n",
    "\\end{cases}\n",
    "\\end{align*}\n",
    "\n",
    "---\n",
    "\n",
    "1. Error Dependence on $\\sigma_0^{NM}$, fix $\\sigma_0^{AM}=0.5$ and vary $\\sigma_0^{NM}=0.1, 0.5, 1.0, 1.5, 2.0, 2.5.$\n",
    "\n",
    "2. Error Dependence on $\\sigma_0^{AM}$, fix $\\sigma_0^{NM}=0.5$ and vary $\\sigma_0^{AM}=0.1, 0.5, 1.0, 1.5, 2.0, 2.5.$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "95200/95200 [==============================] - 37s 391us/step - loss: 0.0407 - mean_squared_error: 0.0407\n",
      "Epoch 2/10\n",
      "95200/95200 [==============================] - 37s 392us/step - loss: 0.0112 - mean_squared_error: 0.0112\n",
      "Epoch 3/10\n",
      "95200/95200 [==============================] - 37s 392us/step - loss: 0.0092 - mean_squared_error: 0.0092\n",
      "Epoch 4/10\n",
      "95200/95200 [==============================] - 37s 392us/step - loss: 0.0077 - mean_squared_error: 0.0077\n",
      "Epoch 5/10\n",
      "95200/95200 [==============================] - 37s 394us/step - loss: 0.0067 - mean_squared_error: 0.0067\n",
      "Epoch 6/10\n",
      "95200/95200 [==============================] - 37s 391us/step - loss: 0.0057 - mean_squared_error: 0.0057\n",
      "Epoch 7/10\n",
      "95200/95200 [==============================] - 37s 391us/step - loss: 0.0051 - mean_squared_error: 0.0051\n",
      "Epoch 8/10\n",
      "95200/95200 [==============================] - 37s 390us/step - loss: 0.0047 - mean_squared_error: 0.0047\n",
      "Epoch 9/10\n",
      "95200/95200 [==============================] - 37s 392us/step - loss: 0.0043 - mean_squared_error: 0.0043\n",
      "Epoch 10/10\n",
      "95200/95200 [==============================] - 37s 391us/step - loss: 0.0040 - mean_squared_error: 0.0040\n",
      "5950/5950 [==============================] - 2s 335us/step - loss: 0.0039 - mean_squared_error: 0.0039\n",
      "For sigma0_AM and sigma0_NM 0.1, the mse errs of df and kf are:5.675%,6.618%\n",
      "Epoch 1/10\n",
      "95200/95200 [==============================] - 36s 382us/step - loss: 0.0442 - mean_squared_error: 0.0442\n",
      "Epoch 2/10\n",
      "95200/95200 [==============================] - 36s 383us/step - loss: 0.0235 - mean_squared_error: 0.0235\n",
      "Epoch 3/10\n",
      "95200/95200 [==============================] - 37s 384us/step - loss: 0.0204 - mean_squared_error: 0.0204\n",
      "Epoch 4/10\n",
      "95200/95200 [==============================] - 37s 384us/step - loss: 0.0190 - mean_squared_error: 0.0190\n",
      "Epoch 5/10\n",
      "95200/95200 [==============================] - 36s 383us/step - loss: 0.0180 - mean_squared_error: 0.0180\n",
      "Epoch 6/10\n",
      "95200/95200 [==============================] - 37s 384us/step - loss: 0.0173 - mean_squared_error: 0.0173\n",
      "Epoch 7/10\n",
      "95200/95200 [==============================] - 37s 384us/step - loss: 0.0168 - mean_squared_error: 0.0168\n",
      "Epoch 8/10\n",
      "95200/95200 [==============================] - 37s 384us/step - loss: 0.0164 - mean_squared_error: 0.0164\n",
      "Epoch 9/10\n",
      "95200/95200 [==============================] - 37s 384us/step - loss: 0.0161 - mean_squared_error: 0.0161\n",
      "Epoch 10/10\n",
      "95200/95200 [==============================] - 37s 386us/step - loss: 0.0157 - mean_squared_error: 0.0157\n",
      "5950/5950 [==============================] - 2s 351us/step - loss: 0.0155 - mean_squared_error: 0.0155\n",
      "For sigma0_AM and sigma0_NM 0.5, the mse errs of df and kf are:1.553%,2.334%\n",
      "Epoch 1/10\n",
      "95200/95200 [==============================] - 37s 387us/step - loss: 0.1134 - mean_squared_error: 0.1134\n",
      "Epoch 2/10\n",
      "95200/95200 [==============================] - 37s 384us/step - loss: 0.0417 - mean_squared_error: 0.0417\n",
      "Epoch 3/10\n",
      "95200/95200 [==============================] - 37s 384us/step - loss: 0.0343 - mean_squared_error: 0.0343\n",
      "Epoch 4/10\n",
      "95200/95200 [==============================] - 36s 382us/step - loss: 0.0310 - mean_squared_error: 0.0310\n",
      "Epoch 5/10\n",
      "95200/95200 [==============================] - 36s 382us/step - loss: 0.0289 - mean_squared_error: 0.0289\n",
      "Epoch 6/10\n",
      "95200/95200 [==============================] - 37s 385us/step - loss: 0.0276 - mean_squared_error: 0.0276\n",
      "Epoch 7/10\n",
      "95200/95200 [==============================] - 37s 385us/step - loss: 0.0268 - mean_squared_error: 0.0268\n",
      "Epoch 8/10\n",
      "95200/95200 [==============================] - 36s 383us/step - loss: 0.0263 - mean_squared_error: 0.0263\n",
      "Epoch 9/10\n",
      "95200/95200 [==============================] - 37s 387us/step - loss: 0.0259 - mean_squared_error: 0.0259\n",
      "Epoch 10/10\n",
      "95200/95200 [==============================] - 37s 390us/step - loss: 0.0256 - mean_squared_error: 0.0256\n",
      "5950/5950 [==============================] - 2s 341us/step - loss: 0.0253 - mean_squared_error: 0.0253\n",
      "For sigma0_AM and sigma0_NM 1.0, the mse errs of df and kf are:4.071%,2.850%\n",
      "Epoch 1/10\n",
      "95200/95200 [==============================] - 37s 387us/step - loss: 0.0940 - mean_squared_error: 0.0940\n",
      "Epoch 2/10\n",
      "95200/95200 [==============================] - 37s 387us/step - loss: 0.0527 - mean_squared_error: 0.0527\n",
      "Epoch 3/10\n",
      "95200/95200 [==============================] - 37s 385us/step - loss: 0.0470 - mean_squared_error: 0.0470\n",
      "Epoch 4/10\n",
      "95200/95200 [==============================] - 37s 387us/step - loss: 0.0443 - mean_squared_error: 0.0443\n",
      "Epoch 5/10\n",
      "95200/95200 [==============================] - 37s 387us/step - loss: 0.0426 - mean_squared_error: 0.0426\n",
      "Epoch 6/10\n",
      "95200/95200 [==============================] - 37s 385us/step - loss: 0.0412 - mean_squared_error: 0.0412\n",
      "Epoch 7/10\n",
      "95200/95200 [==============================] - 37s 385us/step - loss: 0.0398 - mean_squared_error: 0.0398\n",
      "Epoch 8/10\n",
      "95200/95200 [==============================] - 37s 385us/step - loss: 0.0389 - mean_squared_error: 0.0389\n",
      "Epoch 9/10\n",
      "95200/95200 [==============================] - 37s 385us/step - loss: 0.0381 - mean_squared_error: 0.0381\n",
      "Epoch 10/10\n",
      "95200/95200 [==============================] - 37s 385us/step - loss: 0.0375 - mean_squared_error: 0.0375\n",
      "5950/5950 [==============================] - 2s 337us/step - loss: 0.0371 - mean_squared_error: 0.0371\n",
      "For sigma0_AM and sigma0_NM 1.5, the mse errs of df and kf are:7.864%,3.756%\n",
      "Epoch 1/10\n",
      "95200/95200 [==============================] - 38s 397us/step - loss: 0.0911 - mean_squared_error: 0.0911\n",
      "Epoch 2/10\n",
      "95200/95200 [==============================] - 38s 394us/step - loss: 0.0529 - mean_squared_error: 0.0529\n",
      "Epoch 3/10\n",
      "95200/95200 [==============================] - 38s 397us/step - loss: 0.0462 - mean_squared_error: 0.0462\n",
      "Epoch 4/10\n",
      "95200/95200 [==============================] - 38s 395us/step - loss: 0.0422 - mean_squared_error: 0.0422\n",
      "Epoch 5/10\n",
      "95200/95200 [==============================] - 38s 401us/step - loss: 0.0395 - mean_squared_error: 0.0395\n",
      "Epoch 6/10\n",
      "95200/95200 [==============================] - 38s 394us/step - loss: 0.0379 - mean_squared_error: 0.0379\n",
      "Epoch 7/10\n",
      "95200/95200 [==============================] - 38s 396us/step - loss: 0.0368 - mean_squared_error: 0.0368\n",
      "Epoch 8/10\n",
      "95200/95200 [==============================] - 38s 396us/step - loss: 0.0359 - mean_squared_error: 0.0359\n",
      "Epoch 9/10\n",
      "95200/95200 [==============================] - 38s 397us/step - loss: 0.0349 - mean_squared_error: 0.0349\n",
      "Epoch 10/10\n",
      "95200/95200 [==============================] - 38s 396us/step - loss: 0.0340 - mean_squared_error: 0.0340\n",
      "5950/5950 [==============================] - 2s 343us/step - loss: 0.0336 - mean_squared_error: 0.0336\n",
      "For sigma0_AM and sigma0_NM 2.0, the mse errs of df and kf are:7.640%,4.728%\n",
      "Epoch 1/10\n",
      "95200/95200 [==============================] - 37s 388us/step - loss: 0.2253 - mean_squared_error: 0.2253\n",
      "Epoch 2/10\n",
      "95200/95200 [==============================] - 37s 388us/step - loss: 0.1100 - mean_squared_error: 0.1100\n",
      "Epoch 3/10\n",
      "95200/95200 [==============================] - 37s 387us/step - loss: 0.0737 - mean_squared_error: 0.0737\n",
      "Epoch 4/10\n",
      "95200/95200 [==============================] - 37s 388us/step - loss: 0.0602 - mean_squared_error: 0.0602\n",
      "Epoch 5/10\n",
      "95200/95200 [==============================] - 37s 388us/step - loss: 0.0551 - mean_squared_error: 0.0551\n",
      "Epoch 6/10\n",
      "95200/95200 [==============================] - 37s 389us/step - loss: 0.0524 - mean_squared_error: 0.0524\n",
      "Epoch 7/10\n",
      "95200/95200 [==============================] - 37s 390us/step - loss: 0.0505 - mean_squared_error: 0.0505\n",
      "Epoch 8/10\n",
      "95200/95200 [==============================] - 37s 391us/step - loss: 0.0492 - mean_squared_error: 0.0492\n",
      "Epoch 9/10\n",
      "95200/95200 [==============================] - 37s 392us/step - loss: 0.0479 - mean_squared_error: 0.0479\n",
      "Epoch 10/10\n",
      "95200/95200 [==============================] - 37s 389us/step - loss: 0.0465 - mean_squared_error: 0.0465\n",
      "5950/5950 [==============================] - 2s 336us/step - loss: 0.0454 - mean_squared_error: 0.0454\n",
      "For sigma0_AM and sigma0_NM 2.5, the mse errs of df and kf are:20.599%,5.712%\n"
     ]
    }
   ],
   "source": [
    "#---------------------------------------------------------\n",
    "# Robustness Analysis for NM\n",
    "#---------------------------------------------------------\n",
    "def robust_analysis_NM():\n",
    "    sigma0_AM=0.5; sigma0_NM=np.array([0.1,0.5,1.0,1.5,2.0,2.5])\n",
    "    \n",
    "    # For Kalman Filter, we use NM model with recursive formula and Riccati equation.\n",
    "    # But when applying KF, we use observation from AM that is y_n is from AM model.\n",
    "    R0_NM=np.zeros((len(sigma0_NM),dimY,dimY))\n",
    "    \n",
    "    for i in range(len(sigma0_NM)):\n",
    "        R0_NM[i]=sigma0_NM[i]*sigma0_NM[i]*np.eye(dimY)\n",
    "        # First train a DF model with NM noise\n",
    "        datas,labels,x_hats,x_bars,x_raws,y_raws=sample_generator(Q0,R0_NM[i],sigma0_NM[i]) #!!! covariance matrix changed.\n",
    "        model,data_mean,data_std = deep_filtering(datas,labels,x_hats,x_bars,x_raws,y_raws)\n",
    "        \n",
    "        # Second generating samples with AM model nosie\n",
    "        x_AM,y_AM=mc_simulation(F,G,H,u,v,sigma0_AM,N)\n",
    "        # Riccati with NM, observarion with AM\n",
    "        x_hat,x_bar=kalman_filtering(F,G,H,Q0,R0_NM[i],x0,y_AM,N)\n",
    "        \n",
    "        x_AM=x_AM.reshape(N+1,dimX)\n",
    "        y_AM=y_AM.reshape(N+1,dimY)\n",
    "        data_new_AM=np.zeros((N-n0+2,n0,dimY))\n",
    "        for k in range(N-n0+2):\n",
    "            data_new_AM[k]=y_AM[k:k+n0]\n",
    "        data_new_AM=data_new_AM.reshape(N-n0+2,n0*dimY)\n",
    "        data_new_AM=pd.DataFrame(data_new_AM)\n",
    "        # input normalization\n",
    "        data_new_AM=(data_new_AM-data_mean)/data_std # data_mean and data_std come from NM noise.\n",
    "        df_pred=model.predict(data_new_AM)\n",
    "        from sklearn.metrics import mean_squared_error\n",
    "        # Deep Filtering Error\n",
    "        df_mse_err=mean_squared_error(x_AM[n0-1:],df_pred)\n",
    "        x_bar=x_bar.reshape(N+1,dimX)\n",
    "        kf_mse_err=mean_squared_error(x_AM[n0-1:],x_bar[n0-1:])\n",
    "        print(\"For sigma0_AM and sigma0_NM {}, the mse errs of df and kf are:{:.2%},{:.2%}\".format(sigma0_NM[i],df_mse_err, kf_mse_err))\n",
    "        \n",
    "#---------------------------------------------------------\n",
    "# Robustness Analysis for AM\n",
    "#---------------------------------------------------------\n",
    "def robust_analysis_AM():\n",
    "    sigma0_AM=np.array([0.1,0.5,1.0,1.5,2.0,2.5]); sigma0_NM=0.5\n",
    "    \n",
    "    # For Kalman Filter, we use NM model with recursive formula and Riccati equation.\n",
    "    # But when applying KF, we use observation from AM that is y_n is from AM model.\n",
    "    R0_NM=sigma0_NM*sigma0_NM*np.eye(dimY)\n",
    "    R0_AM=np.zeros((len(sigma0_AM),dimY,dimY))\n",
    "    \n",
    "    for i in range(len(sigma0_AM)):\n",
    "        R0_AM[i]=sigma0_AM[i]*sigma0_AM[i]*np.eye(dimY)\n",
    "        \n",
    "        # First train a DF model with NM noise\n",
    "        datas,labels,x_hats,x_bars,x_raws,y_raws=sample_generator(Q0,R0_NM,sigma0_NM)\n",
    "        model,data_mean,data_std = deep_filtering(datas,labels,x_hats,x_bars,x_raws,y_raws)\n",
    "        \n",
    "        # Second generating samples with AM model nosie\n",
    "        x_AM,y_AM=mc_simulation(F,G,H,u,v,sigma0_AM[i],N)\n",
    "        # Riccati with NM, observarion with AM\n",
    "        x_hat,x_bar=kalman_filtering(F,G,H,Q0,R0_NM,x0,y_AM,N)\n",
    "        \n",
    "        x_AM=x_AM.reshape(N+1,dimX)\n",
    "        y_AM=y_AM.reshape(N+1,dimY)\n",
    "        data_new_AM=np.zeros((N-n0+2,n0,dimY))\n",
    "        for k in range(N-n0+2):\n",
    "            data_new_AM[k]=y_AM[k:k+n0]\n",
    "        data_new_AM=data_new_AM.reshape(N-n0+2,n0*dimY)\n",
    "        data_new_AM=pd.DataFrame(data_new_AM)\n",
    "        # input normalization\n",
    "        data_new_AM=(data_new_AM-data_mean)/data_std\n",
    "        df_pred=model.predict(data_new_AM)\n",
    "        from sklearn.metrics import mean_squared_error\n",
    "        # Deep Filtering Error\n",
    "        df_mse_err=mean_squared_error(x_AM[n0-1:],df_pred)\n",
    "        x_bar=x_bar.reshape(N+1,dimX)\n",
    "        kf_mse_err=mean_squared_error(x_AM[n0-1:],x_bar[n0-1:])\n",
    "        print(\"For fixed sigma0_NM and sigma0_AM {}, the mse errs of df and kf are:{:.2%},{:.2%}\".format(sigma0_AM[i],df_mse_err, kf_mse_err))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call this function\n",
    "robust_analysis_NM()\n",
    "robust_analysis_AM()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
